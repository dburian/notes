[b/000001]: bibliography/000001.md

# Gradient decent

Gradient decent je optimalizační technika pro neuronové sítě. Spočívá v
minimalizování funkce chyby (error/objective function) neuronové sítě pro daný
vstup a chtěný výstup.

Mějme krok učícího algoritmu pro neuronovou síť, která přijímá na vstupu vektor
$\textbf{x}$, ke kterému vypočítá výstup $\textbf{y}$ a chce dosáhnout výstupu
$\textbf{d}$. Definujme funkci $E(\textbf{y}, \textbf{d})$, která
určuje chybu výpočtu. Tuto funkci bychom rádi zminimalizovali (pokud možno na
0).

Zvolíme-li si šikovně $E$ a aktivační funkce neuronů, je $E$ libovolně
derivovatelná. Myšlenka optimalizační techniky gradient decent je určit sklon
funkce $E$ v bodě $(w_{11}, w_{12}, \ldots)^T$ a posunout váhy směrem k
lokálnímu minimu funkce $E$. Tedy v každé iteraci odečteme od každé váhy
parciální derivaci funkce $E$ vzhledem k dané váze. Abychom měli větší kontrolu
nad změnami vah přidává se ještě konstanta $\alpha$, které se říká učící faktor
(learning rate). Pokud bychom označili $w_{ij}^{t}$ váhu po $t$ iteraci, můžeme
gradient decent shrnout na:

$$
w_{ij}^{t+1} =
w_{ij}^t - \alpha \dot
\frac{\partial E(\textbf{y}, \textbf{d})}{\partial w_{ij}}
$$

Gradient decent je velmi jednoduchý algoritmus, má však řadu nevýhod:
- poměrně pomalý
- hledá lokální minimum, né globální,
- snadno skáče z místa na místo.

Zdroj: [NAIL002 Neuronové sítě][b/000001]
