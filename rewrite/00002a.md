[00002b]: 00002b.md
[b/000001]: bibliography/000001.md

# Technika zpětná propagace (back-propagation)

Technika zpětné propagace ukazuje jak efektivně počítat [optimalizační techniku
gradient decent][00002b]. Idea je, že gradient chybové funkce snadno spočítáme
pro poslední vrstvu neuronů. Pro předposlední vrstvu se díky použití pravidla
derivace pro vnořené funkce využije výsledku výpočtu pro poslední vrstvu. Tedy
změna vah se počítá postupně od nejposlednější vrstvy až po tu první.

Konkrétně pro chybovou funkci definovanou jako:

$$
E(\textbf{y}, \textbf{d}) = \frac{1}{2} \sum_{k} (\textbf{y}_k - \textbf{d}_k)^2
$$

a pro sigmoidové aktivační funkce pro všechny neurony by váha mezi $i$ a $j$ tým
neuronem v $t + 1$ iteraci učícího algoritmu vypadala takto:

$$
w_{ij}^{t+1} = w_{ij}^t + \alpha \delta_j \textbf{y}_i
$$
, kde $\textbf{y}_i$ je výstup $i$ neuronu a

- $\delta_j = (\textbf{d}_j - \textbf{y}_j) \lambda \textbf{y}_j (1 -$
  $\textbf{y}_j)$ pro $j$ neuron v poslední vrstvě
- $\delta_j = \big(\sum_{i} \delta_i w_{ji} \big) \lambda \textbf{y}_j$
  $(1 - \textbf{y}_j)$ pro $j$ neuron v mezivrstvě.

Zdroj: [NAIL002 Neuronové sítě][b/000001]
