---
tags: [ml,transformers]
---
[paper]: https://arxiv.org/pdf/1910.10683.pdf
[transformer_paper]: https://arxiv.org/pdf/1706.03762.pdf
[rel_pos_embed_paper]: 

# Text-To-Text-Transfer-Transformer (T5)

T5 transformer model comes from a [paper][paper] whose aim was to unify
NLP pre-training tasks. The authors achieved this by creating text-to-text
model and by re-framing pre-training tasks as text-to-text tasks.

The paper also introduced new English text corpus named *Colossal Clean Crawled
Corpus* (C4). The large (750GB) corpus originated from Common Crawl but was very
heavily cleaned.


## The model

The model closely follows original [Transformer][transformer_paper]
encoder-decoder architecture with few exceptions:

- simplified [relative positional embeddings][rel_pos_embed_paper] are used instead
of absolute positional embeddings
- laynorm layer is outside of the residual
path (TODO: image here) and no bias is added


The model uses WordPiece tokens that include English, German, French and
Romanian.

### Training

For pre-training the authors used a variant of masked language modelling for
encoder-decoder architectures.





